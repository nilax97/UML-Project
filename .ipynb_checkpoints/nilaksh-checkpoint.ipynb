{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d531a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import struct\n",
    "import hashlib\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "import tensorflow_hub as hub\n",
    "import tokenization\n",
    "from sklearn.manifold import TSNE\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e1f9af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287227\n",
      "13368\n",
      "11490\n"
     ]
    }
   ],
   "source": [
    "## Generate list of training/validation and test files\n",
    "\n",
    "def hashhex(s):\n",
    "    h = hashlib.sha1()\n",
    "    h.update(s)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def get_url_hashes(url_list):\n",
    "    return [hashhex(url) for url in url_list]\n",
    "\n",
    "def read_text_file(text_file):\n",
    "    lines = []\n",
    "    with open(text_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            lines.append(line.strip().encode('utf-8'))\n",
    "    return lines\n",
    "\n",
    "url_file_train = 'CNN-DM/all_train.txt'\n",
    "url_list_train = read_text_file(url_file_train)\n",
    "url_hashes_train = get_url_hashes(url_list_train)\n",
    "print(len(url_hashes_train))\n",
    "\n",
    "url_file_val = 'CNN-DM/all_val.txt'\n",
    "url_list_val = read_text_file(url_file_val)\n",
    "url_hashes_val = get_url_hashes(url_list_val)\n",
    "print(len(url_hashes_val))\n",
    "\n",
    "url_file_test = 'CNN-DM/all_test.txt'\n",
    "url_list_test = read_text_file(url_file_test)\n",
    "url_hashes_test = get_url_hashes(url_list_test)\n",
    "print(len(url_hashes_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fbdb3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Line by line preprocessing (no tokenization)\n",
    "def preprocess(x):\n",
    "    return x.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b415b8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean text, and split summary\n",
    "def clean(x):\n",
    "    text = []\n",
    "    summary = []\n",
    "    flag = 0\n",
    "    for line in x:\n",
    "        if line == '\\n':\n",
    "            continue\n",
    "        if '@highlight' in line:\n",
    "            flag = 1\n",
    "            continue\n",
    "        if flag == 0:\n",
    "            text.append(preprocess(line))\n",
    "        else:\n",
    "            summary.append(preprocess(line))\n",
    "    return text,summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e86d4702",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model features include an encode function -> takes a list of sentences. Returns a list of embeddings (all same dim)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5740baad",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2','rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d7b4dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## V = list of embeddings. k = target size of summary\n",
    "## Returns a list of sentence indices\n",
    "\n",
    "def generate_summary(V, k):\n",
    "    k -= 1\n",
    "    centers = []\n",
    "    cities = list(range(len(V)))\n",
    "    centers.append(0)\n",
    "    cities.remove(0)\n",
    "    while k!= 0:\n",
    "        city_dict = {}\n",
    "        for cty in cities:\n",
    "            min_dist = float(\"inf\")\n",
    "            for c in centers:\n",
    "                min_dist = min(min_dist,np.linalg.norm(V[cty] - V[c]))\n",
    "            city_dict[cty] = min_dist\n",
    "        new_center = max(city_dict, key = lambda i: city_dict[i])\n",
    "        centers.append(new_center)\n",
    "        cities.remove(new_center)\n",
    "        k -= 1\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ca2f5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Return R1, R2 and RL score for a text (using the generate_summary function)\n",
    "\n",
    "def uml_summary(l):\n",
    "    text,summary = clean(l)\n",
    "    text_emb = model.encode(text)\n",
    "    gen_sum = [text[x] for x in generate_summary(text_emb,len(summary))]\n",
    "    scores = scorer.score(\" \".join(summary),\" \".join(gen_sum))\n",
    "    return scores[\"rouge1\"].fmeasure, scores[\"rouge2\"].fmeasure, scores[\"rougeL\"].fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec729e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge 1 :  30.36\n",
      "Rouge 2 :  11.64\n",
      "Rouge L :  22.03\n"
     ]
    }
   ],
   "source": [
    "r1 = []\n",
    "r2 = []\n",
    "rl = []\n",
    "for i in range(len(url_hashes_test)):\n",
    "    if i>0 and i%100 == 0:\n",
    "        print(i,\"Done\")\n",
    "    x = url_hashes_test[i]\n",
    "    if os.path.exists(os.path.join(\"CNN-DM\",\"raw\",\"cnn\",\"stories\",x+\".story\")):\n",
    "        l = open(os.path.join(\"CNN-DM\",\"raw\",\"cnn\",\"stories\",x+\".story\")).readlines()\n",
    "    else:\n",
    "        l = open(os.path.join(\"CNN-DM\",\"raw\",\"dm\",\"stories\",x+\".story\")).readlines()\n",
    "    r1_val,r2_val,rl_val = uml_summary(l)\n",
    "    r1.append(r1_val)\n",
    "    r2.append(r2_val)\n",
    "    rl.append(rl_val)\n",
    "print(\"Rouge 1 : \",np.round(np.mean(np.asarray(r1))*100,2))\n",
    "print(\"Rouge 2 : \",np.round(np.mean(np.asarray(r2))*100,2))\n",
    "print(\"Rouge L : \",np.round(np.mean(np.asarray(rl))*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b40ee8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19cb9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751ce030",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
