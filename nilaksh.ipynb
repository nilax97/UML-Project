{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d531a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import struct\n",
    "import hashlib\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "import tensorflow_hub as hub\n",
    "import tokenization\n",
    "from sklearn.manifold import TSNE\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.cluster import KMeans\n",
    "import pickle\n",
    "from sknetwork.ranking import PageRank\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e443f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df4bb4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /mnt/disks/disk-1/data/cnn_dailymail/3.1.0\n",
      "INFO:absl:Reusing dataset cnn_dailymail (/mnt/disks/disk-1/data/cnn_dailymail/3.1.0)\n",
      "INFO:absl:Constructing tf.data.Dataset cnn_dailymail for split ['train', 'validation', 'test'], from /mnt/disks/disk-1/data/cnn_dailymail/3.1.0\n",
      "2021-11-28 05:39:26.504796: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-28 05:39:26.995786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38464 MB memory:  -> device: 0, name: A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "cnn_dm_train, cnn_dm_val, cnn_dm_test = tfds.load(name=\"cnn_dailymail\", \n",
    "                                                  split=[\"train\", \"validation\", \"test\"], \n",
    "                                                  data_dir=\"/mnt/disks/disk-1/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1ff246",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_train, arxiv_val, arxiv_test = tfds.load(name=\"scientific_papers/arxiv\", \n",
    "                                                  split=[\"train\", \"validation\", \"test\"], \n",
    "                                                  data_dir=\"/mnt/disks/disk-1/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf43ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed_train, pubmed_val, pubmed_test = tfds.load(name=\"scientific_papers/pubmed\", \n",
    "                                                  split=[\"train\", \"validation\", \"test\"], \n",
    "                                                  data_dir=\"/mnt/disks/disk-1/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8058edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = cnn_dm_train.take(1)\n",
    "ds = arxiv_train.take(1)\n",
    "# ds = pubmed_train.take(1)\n",
    "for example in ds:  # example is `{'image': tf.Tensor, 'label': tf.Tensor}`\n",
    "    print(list(example.keys()))\n",
    "    x = example[\"article\"].numpy().decode('UTF-8').split('\\n')\n",
    "    for l in x:\n",
    "        print(l)\n",
    "        print(\"_\")\n",
    "    print(\"---\")\n",
    "    x = example[\"abstract\"].numpy().decode('UTF-8').split('\\n')\n",
    "    for l in x:\n",
    "        print(l)\n",
    "        print(\"_\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2141e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_string(x):\n",
    "    return x.numpy().decode('UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "319e7aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_list(text,stem=None):\n",
    "    sents = sent_tokenize(text)\n",
    "    if stem is None:\n",
    "        return sents\n",
    "    ans = []\n",
    "    for sent in sents:\n",
    "        words = word_tokenize(sent)\n",
    "        word_stem = [stem.stem(w) for w in words]\n",
    "        ans.append(\" \".join(word_stem))\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e1f9af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-28 05:39:38.098516: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "x = list(cnn_dm_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fbdb3a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['expert question if pack out plane are put passeng at risk .',\n",
       " 'u. consum advisori group say minimum space must be stipul .',\n",
       " 'safeti test conduct on plane with more leg room than airlin offer .']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = EnglishStemmer()\n",
    "get_sent_list(tensor_to_string(x['highlights']),stem=stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b415b8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uml_summary(x,kind=\"cnn_dm\",stem=None,model=None):\n",
    "    if kind == \"cnn_dm\":\n",
    "        key1 = 'article'\n",
    "        key2 = 'highlights'\n",
    "    elif kind == 'arxiv' or kind == 'pubmed':\n",
    "        key1 = 'article'\n",
    "        key2 = 'abstract'\n",
    "    text = tensor_to_string(x[key1])\n",
    "    text = get_sent_list(text,stem)\n",
    "    summary = tensor_to_string(x[key2])\n",
    "    summary = get_sent_list(summary,stem)\n",
    "    text_emb = model.encode(text)\n",
    "    filename = \n",
    "    with open('filename.pickle', 'wb') as handle:\n",
    "        pickle.dump(a, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    gen_sum = [text[x] for x in generate_summary(text_emb,len(summary))]\n",
    "    scores = scorer.score(\" \".join(summary),\" \".join(gen_sum))\n",
    "    return scores[\"rouge1\"].fmeasure, scores[\"rouge2\"].fmeasure, scores[\"rougeL\"].fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e86d4702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9235c7c49a1c4e94b993c980c0c8e865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93f9ceef978414cb94cce23e6cc2b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/10.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d318c74d27f34677b862bb716b9bbd23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca0f64ec02f4e9e825bcd284e2a6041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6867fe6023c49d0889765e7c57ca801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a107fb5fdf45028aded989c885893a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429ed60482f044ee8743be68ba689af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f628d4ebd84cdfa074a96fa684657b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0790588184be47ec8ab06ba7f61cf767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a6a032d2f134861b7a377a71ef16d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb18ed283b04d90a39a6510f09d2421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c59dcac2e0244bba815debd64ebd4edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef936d70cb64b10b6bcd5a199848e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea468b689ba4eb29b337600b8d70472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Model features include an encode function -> takes a list of sentences. Returns a list of embeddings (all same dim)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2',cache_folder='/mnt/disks/disk-1/data/models')\n",
    "\n",
    "transformers = [\"all-mpnet-base-v2\",\n",
    "                \"multi-qa-mpnet-base-dot-v1\",\n",
    "                \"all-distilroberta-v1\",\n",
    "                \"all-MiniLM-L12-v2\",\n",
    "                \"multi-qa-distilbert-cos-v1\",\n",
    "                \"all-MiniLM-L6-v2\",\n",
    "                \"multi-qa-MiniLM-L6-cos-v1\",\n",
    "                \"paraphrase-multilingual-mpnet-base-v2\",\n",
    "                \"paraphrase-albert-small-v2\",\n",
    "                \"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "                \"paraphrase-MiniLM-L3-v2\",\n",
    "                \"distiluse-base-multilingual-cased-v1\",\n",
    "                \"distiluse-base-multilingual-cased-v2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5740baad",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2','rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7b4dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## V = list of embeddings. k = target size of summary\n",
    "## Returns a list of sentence indices\n",
    "\n",
    "def generate_summary(V, k):\n",
    "    if k >= len(V):\n",
    "        return list(range(len(V)))\n",
    "    k -= 1\n",
    "    centers = []\n",
    "    cities = list(range(len(V)))\n",
    "    centers.append(0)\n",
    "    cities.remove(0)\n",
    "    while k!= 0:\n",
    "        city_dict = {}\n",
    "        for cty in cities:\n",
    "            min_dist = float(\"inf\")\n",
    "            for c in centers:\n",
    "                min_dist = min(min_dist,np.linalg.norm(V[cty] - V[c]))\n",
    "            city_dict[cty] = min_dist\n",
    "        new_center = max(city_dict, key = lambda i: city_dict[i])\n",
    "        centers.append(new_center)\n",
    "        cities.remove(new_center)\n",
    "        k -= 1\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fc014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pagerank version\n",
    "def sim(a, b):\n",
    "    return np.dot(a, b) / np.sqrt(np.dot(a, a) * np.dot(b, b))\n",
    "\n",
    "def generate_summary(V,k):\n",
    "    if k >= len(V):\n",
    "        return list(range(len(V)))\n",
    "    n = V.shape[0]\n",
    "    adj = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        adj[i][i] = sim(V[i],V[i])\n",
    "        for j in range(i+1,n):\n",
    "            s = sim(V[i], V[j])\n",
    "            adj[i][j] = s\n",
    "            adj[j][i] = s\n",
    "\n",
    "    pr = PageRank()\n",
    "    scores = pr.fit_transform(adj)\n",
    "    ind = np.argpartition(scores, -k)[-k:]\n",
    "    return np.sort(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca2f5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Return R1, R2 and RL score for a text (using the generate_summary function)\n",
    "\n",
    "def uml_summary(l):\n",
    "    text,summary = clean(l)\n",
    "    text_emb = model.encode(text)\n",
    "    gen_sum = [text[x] for x in generate_summary(text_emb,len(summary))]\n",
    "    scores = scorer.score(\" \".join(summary),\" \".join(gen_sum))\n",
    "    return scores[\"rouge1\"].fmeasure, scores[\"rouge2\"].fmeasure, scores[\"rougeL\"].fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec729e22",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Don't run this\n",
    "### all-MiniLM-L6-v2 with K-Centers\n",
    "r1 = []\n",
    "r2 = []\n",
    "rl = []\n",
    "for i in range(len(url_hashes_val)):\n",
    "    if i>0 and i%1000 == 0:\n",
    "        print(i,\"Done\")\n",
    "    x = url_hashes_val[i]\n",
    "    if os.path.exists(os.path.join(\"CNN-DM\",\"raw\",\"cnn\",\"stories\",x+\".story\")):\n",
    "        l = open(os.path.join(\"CNN-DM\",\"raw\",\"cnn\",\"stories\",x+\".story\")).readlines()\n",
    "    else:\n",
    "        l = open(os.path.join(\"CNN-DM\",\"raw\",\"dailymail\",\"stories\",x+\".story\")).readlines()\n",
    "    r1_val,r2_val,rl_val = uml_summary(l)\n",
    "    r1.append(r1_val)\n",
    "    r2.append(r2_val)\n",
    "    rl.append(rl_val)\n",
    "\n",
    "# (Uncomment to test for 1 line)\n",
    "#     break\n",
    "print(\"Rouge 1 : \",np.round(np.mean(np.asarray(r1))*100,2))\n",
    "print(\"Rouge 2 : \",np.round(np.mean(np.asarray(r2))*100,2))\n",
    "print(\"Rouge L : \",np.round(np.mean(np.asarray(rl))*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b40ee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Don't run this\n",
    "### all-MiniLM-L6-v2 with PageRank\n",
    "r1 = []\n",
    "r2 = []\n",
    "rl = []\n",
    "for i in range(len(url_hashes_val)):\n",
    "    if i>0 and i%1000 == 0:\n",
    "        print(i,\"Done\")\n",
    "    x = url_hashes_val[i]\n",
    "    if os.path.exists(os.path.join(\"CNN-DM\",\"raw\",\"cnn\",\"stories\",x+\".story\")):\n",
    "        l = open(os.path.join(\"CNN-DM\",\"raw\",\"cnn\",\"stories\",x+\".story\")).readlines()\n",
    "    else:\n",
    "        l = open(os.path.join(\"CNN-DM\",\"raw\",\"dailymail\",\"stories\",x+\".story\")).readlines()\n",
    "    r1_val,r2_val,rl_val = uml_summary(l)\n",
    "    r1.append(r1_val)\n",
    "    r2.append(r2_val)\n",
    "    rl.append(rl_val)\n",
    "\n",
    "# (Uncomment to test for 1 line)\n",
    "#     break\n",
    "print(\"Rouge 1 : \",np.round(np.mean(np.asarray(r1))*100,2))\n",
    "print(\"Rouge 2 : \",np.round(np.mean(np.asarray(r2))*100,2))\n",
    "print(\"Rouge L : \",np.round(np.mean(np.asarray(rl))*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c4a7ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751ce030",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\".join(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b60eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text,summary = clean(l)\n",
    "print(\"\\n\".join(text))\n",
    "print(\"___\")\n",
    "print(\"\\n\".join(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1129fe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cb64c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e28a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4dbe2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9966f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d44cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1497f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
