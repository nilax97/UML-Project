{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d531a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import struct\n",
    "import hashlib\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "import tensorflow_hub as hub\n",
    "import tokenization\n",
    "from sklearn.manifold import TSNE\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.cluster import KMeans\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e1f9af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287227\n",
      "13368\n",
      "11490\n"
     ]
    }
   ],
   "source": [
    "## Generate list of training/validation and test files\n",
    "\n",
    "def hashhex(s):\n",
    "    h = hashlib.sha1()\n",
    "    h.update(s)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def get_url_hashes(url_list):\n",
    "    return [hashhex(url) for url in url_list]\n",
    "\n",
    "def read_text_file(text_file):\n",
    "    lines = []\n",
    "    with open(text_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            lines.append(line.strip().encode('utf-8'))\n",
    "    return lines\n",
    "\n",
    "url_file_train = 'CNN-DM/all_train.txt'\n",
    "url_list_train = read_text_file(url_file_train)\n",
    "url_hashes_train = get_url_hashes(url_list_train)\n",
    "print(len(url_hashes_train))\n",
    "\n",
    "url_file_val = 'CNN-DM/all_val.txt'\n",
    "url_list_val = read_text_file(url_file_val)\n",
    "url_hashes_val = get_url_hashes(url_list_val)\n",
    "print(len(url_hashes_val))\n",
    "\n",
    "url_file_test = 'CNN-DM/all_test.txt'\n",
    "url_list_test = read_text_file(url_file_test)\n",
    "url_hashes_test = get_url_hashes(url_list_test)\n",
    "print(len(url_hashes_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fbdb3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Line by line preprocessing (no tokenization)\n",
    "def preprocess(x):\n",
    "    return x.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b415b8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean text, and split summary\n",
    "def clean(x):\n",
    "    text = []\n",
    "    summary = []\n",
    "    flag = 0\n",
    "    for line in x:\n",
    "        if line == '\\n':\n",
    "            continue\n",
    "        if '@highlight' in line:\n",
    "            flag = 1\n",
    "            continue\n",
    "        if flag == 0:\n",
    "            text.append(preprocess(line))\n",
    "        else:\n",
    "            summary.append(preprocess(line))\n",
    "    return text,summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e86d4702",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model features include an encode function -> takes a list of sentences. Returns a list of embeddings (all same dim)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5740baad",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2','rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d7b4dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## V = list of embeddings. k = target size of summary\n",
    "## Returns a list of sentence indices\n",
    "\n",
    "def generate_summary(V, k):\n",
    "    if k >= len(V):\n",
    "        return list(range(len(V)))\n",
    "    k -= 1\n",
    "    centers = []\n",
    "    cities = list(range(len(V)))\n",
    "    centers.append(0)\n",
    "    cities.remove(0)\n",
    "    while k!= 0:\n",
    "        city_dict = {}\n",
    "        for cty in cities:\n",
    "            min_dist = float(\"inf\")\n",
    "            for c in centers:\n",
    "                min_dist = min(min_dist,np.linalg.norm(V[cty] - V[c]))\n",
    "            city_dict[cty] = min_dist\n",
    "        new_center = max(city_dict, key = lambda i: city_dict[i])\n",
    "        centers.append(new_center)\n",
    "        cities.remove(new_center)\n",
    "        k -= 1\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ca2f5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Return R1, R2 and RL score for a text (using the generate_summary function)\n",
    "\n",
    "def uml_summary(l):\n",
    "    text,summary = clean(l)\n",
    "    text_emb = model.encode(text)\n",
    "    gen_sum = [text[x] for x in generate_summary(text_emb,len(summary))]\n",
    "    scores = scorer.score(\" \".join(summary),\" \".join(gen_sum))\n",
    "    return scores[\"rouge1\"].fmeasure, scores[\"rouge2\"].fmeasure, scores[\"rougeL\"].fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec729e22",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Done\n",
      "200 Done\n",
      "300 Done\n",
      "400 Done\n",
      "500 Done\n",
      "600 Done\n",
      "700 Done\n",
      "800 Done\n",
      "900 Done\n",
      "1000 Done\n",
      "1100 Done\n",
      "1200 Done\n",
      "1300 Done\n",
      "1400 Done\n",
      "1500 Done\n",
      "1600 Done\n",
      "1700 Done\n",
      "1800 Done\n",
      "1900 Done\n",
      "2000 Done\n",
      "2100 Done\n",
      "2200 Done\n",
      "2300 Done\n",
      "2400 Done\n",
      "2500 Done\n",
      "2600 Done\n",
      "2700 Done\n",
      "2800 Done\n",
      "2900 Done\n",
      "3000 Done\n",
      "3100 Done\n",
      "3200 Done\n",
      "3300 Done\n",
      "3400 Done\n",
      "3500 Done\n",
      "3600 Done\n",
      "3700 Done\n",
      "3800 Done\n",
      "3900 Done\n",
      "4000 Done\n",
      "4100 Done\n",
      "4200 Done\n",
      "4300 Done\n",
      "4400 Done\n",
      "4500 Done\n",
      "4600 Done\n",
      "4700 Done\n",
      "4800 Done\n",
      "4900 Done\n",
      "5000 Done\n",
      "5100 Done\n",
      "5200 Done\n",
      "5300 Done\n",
      "5400 Done\n",
      "5500 Done\n",
      "5600 Done\n",
      "5700 Done\n",
      "5800 Done\n",
      "5900 Done\n",
      "6000 Done\n",
      "6100 Done\n",
      "6200 Done\n",
      "6300 Done\n",
      "6400 Done\n",
      "6500 Done\n",
      "6600 Done\n",
      "6700 Done\n",
      "6800 Done\n",
      "6900 Done\n",
      "7000 Done\n",
      "7100 Done\n",
      "7200 Done\n",
      "7300 Done\n",
      "7400 Done\n",
      "7500 Done\n",
      "7600 Done\n",
      "7700 Done\n",
      "7800 Done\n",
      "7900 Done\n",
      "8000 Done\n",
      "8100 Done\n",
      "8200 Done\n",
      "8300 Done\n",
      "8400 Done\n",
      "8500 Done\n",
      "8600 Done\n",
      "8700 Done\n",
      "8800 Done\n",
      "8900 Done\n",
      "9000 Done\n",
      "9100 Done\n",
      "9200 Done\n",
      "9300 Done\n",
      "9400 Done\n",
      "9500 Done\n",
      "9600 Done\n",
      "9700 Done\n",
      "9800 Done\n",
      "9900 Done\n",
      "10000 Done\n",
      "10100 Done\n",
      "10200 Done\n",
      "10300 Done\n",
      "10400 Done\n",
      "10500 Done\n",
      "10600 Done\n",
      "10700 Done\n",
      "10800 Done\n",
      "10900 Done\n",
      "11000 Done\n",
      "11100 Done\n",
      "11200 Done\n",
      "11300 Done\n",
      "11400 Done\n",
      "Rouge 1 :  30.56\n",
      "Rouge 2 :  9.44\n",
      "Rouge L :  18.62\n"
     ]
    }
   ],
   "source": [
    "# Don't run this\n",
    "\n",
    "r1 = []\n",
    "r2 = []\n",
    "rl = []\n",
    "for i in range(len(url_hashes_test)):\n",
    "    if i>0 and i%100 == 0:\n",
    "        print(i,\"Done\")\n",
    "    x = url_hashes_test[i]\n",
    "    if os.path.exists(os.path.join(\"CNN-DM\",\"raw\",\"cnn\",\"stories\",x+\".story\")):\n",
    "        l = open(os.path.join(\"CNN-DM\",\"raw\",\"cnn\",\"stories\",x+\".story\")).readlines()\n",
    "    else:\n",
    "        l = open(os.path.join(\"CNN-DM\",\"raw\",\"dailymail\",\"stories\",x+\".story\")).readlines()\n",
    "    r1_val,r2_val,rl_val = uml_summary(l)\n",
    "    r1.append(r1_val)\n",
    "    r2.append(r2_val)\n",
    "    rl.append(rl_val)\n",
    "print(\"Rouge 1 : \",np.round(np.mean(np.asarray(r1))*100,2))\n",
    "print(\"Rouge 2 : \",np.round(np.mean(np.asarray(r2))*100,2))\n",
    "print(\"Rouge L : \",np.round(np.mean(np.asarray(rl))*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b40ee8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19cb9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751ce030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "18b60eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1129fe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = url_hashes_test[0]\n",
    "l = open(os.path.join(\"CNN-DM\",\"raw\",\"cnn\",\"stories\",x+\".story\")).readlines()\n",
    "text,summary = clean(l)\n",
    "text_embeddings = model.encode(text)\n",
    "summary_embeddings = model.encode(summary)\n",
    "\n",
    "kmeans = KMeans(n_clusters=len(summary_embeddings)).fit(text_embeddings)\n",
    "min_dist = [float('inf')] * len(summary_embeddings)\n",
    "min_node = [-1] * len(summary_embeddings)\n",
    "\n",
    "for i in range(len(text_embeddings)):\n",
    "    cls = kmeans.labels_[i]\n",
    "    dist = np.linalg.norm(text_embeddings[i] - kmeans.cluster_centers_[cls])\n",
    "    if dist < min_dist[cls]:\n",
    "        min_node[cls] = i\n",
    "        min_dist[cls] = dist\n",
    "gen_summary = [text[x] for x in min_node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cb64c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
