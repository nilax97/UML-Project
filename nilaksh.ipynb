{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d531a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import struct\n",
    "import hashlib\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "import tensorflow_hub as hub\n",
    "import tokenization\n",
    "from sklearn.manifold import TSNE\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.cluster import KMeans\n",
    "import pickle\n",
    "from sknetwork.ranking import PageRank, HITS\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import torch\n",
    "import sys\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2141e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_string(x):\n",
    "    return x.numpy().decode('UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "319e7aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_list(text,stem=None):\n",
    "    sents = sent_tokenize(text)\n",
    "    if stem == \"None\":\n",
    "        return sents\n",
    "    if stem == \"EnglishStemmer\":\n",
    "        stemmer = EnglishStemmer()\n",
    "    ans = []\n",
    "    for sent in sents:\n",
    "        words = word_tokenize(sent)\n",
    "        word_stem = [stemmer.stem(w) for w in words]\n",
    "        ans.append(str(\" \".join(word_stem)))\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbdb3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e86d4702",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Model features include an encode function -> takes a list of sentences. Returns a list of embeddings (all same dim)\n",
    "transformers = [\"multi-qa-mpnet-base-dot-v1\"]\n",
    "\n",
    "models = dict()\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for trans in transformers:\n",
    "    models[trans] = SentenceTransformer(trans,cache_folder='/mnt/disks/disk-1/data/models')\n",
    "    models[trans]._target_device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d7b4dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## V = list of embeddings. k = target size of summary\n",
    "## Returns a list of sentence indices\n",
    "\n",
    "def generate_summary(V, k):\n",
    "    if k >= len(V):\n",
    "        return list(range(len(V)))\n",
    "    k -= 1\n",
    "    centers = []\n",
    "    cities = list(range(len(V)))\n",
    "    centers.append(0)\n",
    "    cities.remove(0)\n",
    "    while k!= 0:\n",
    "        city_dict = {}\n",
    "        for cty in cities:\n",
    "            min_dist = float(\"inf\")\n",
    "            for c in centers:\n",
    "                min_dist = min(min_dist,np.linalg.norm(V[cty] - V[c]))\n",
    "            city_dict[cty] = min_dist\n",
    "        new_center = max(city_dict, key = lambda i: city_dict[i])\n",
    "        centers.append(new_center)\n",
    "        cities.remove(new_center)\n",
    "        k -= 1\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90fc014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pagerank version\n",
    "\n",
    "def sim(a, b):\n",
    "    return ((np.dot(a, b) / np.sqrt(np.dot(a, a) * np.dot(b, b))) + 1) / 2\n",
    "\n",
    "def generate_summary(V,k):\n",
    "    if k >= len(V):\n",
    "        return list(range(len(V)))\n",
    "    n = V.shape[0]\n",
    "    adj = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        adj[i][i] = sim(V[i],V[i])\n",
    "        for j in range(i+1,n):\n",
    "            s = sim(V[i], V[j])\n",
    "            adj[i][j] = s\n",
    "            adj[j][i] = s\n",
    "            \n",
    "    g = nx.complete_graph(len(lst))\n",
    "    adj = np.zeros((len(lst), len(lst)))\n",
    "    for i in range(len(lst)):\n",
    "        for j in range(i, len(lst)):\n",
    "            s = sim(lst[i], lst[j])\n",
    "            g.edges[i,j]['weight'] = s\n",
    "\n",
    "    pr_pair = sorted(\n",
    "        nx.pagerank(g).items(),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:k]\n",
    "\n",
    "    return np.array([k for k, _ in pr_pair])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e813da05",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HITS version\n",
    "\n",
    "def sim(a, b):\n",
    "    return ((np.dot(a, b) / np.sqrt(np.dot(a, a) * np.dot(b, b))) + 1) / 2\n",
    "\n",
    "def generate_summary_hits(V,k,threshold=0.5):\n",
    "    if k >= len(V):\n",
    "        return list(range(len(V)))\n",
    "    n = V.shape[0]\n",
    "    adj = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        adj[i][i] = sim(V[i],V[i])\n",
    "        for j in range(i+1,n):\n",
    "            s = 0 if sim(V[i], V[j]) < threshold else 1\n",
    "            adj[i][j] = s\n",
    "            adj[j][i] = s\n",
    "            \n",
    "    pr = HITS()\n",
    "    scores = pr.fit_transform(adj)\n",
    "    ind = np.argpartition(scores, -k)[-k:]\n",
    "    return np.sort(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b415b8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uml_summary(x,index,kind=\"cnn_dailymail\",model=\"multi-qa-mpnet-base-dot-v1\"):\n",
    "    if kind == \"cnn_dailymail\":\n",
    "        key1 = 'article'\n",
    "        key2 = 'highlights'\n",
    "    elif kind == \"scientific_papers/arxiv\" or kind == \"scientific_papers/pubmed\":\n",
    "        key1 = 'article'\n",
    "        key2 = 'abstract'\n",
    "        \n",
    "    stemmer = \"EnglishStemmer\"\n",
    "    text = tensor_to_string(x[key1])\n",
    "    text = get_sent_list(text,stemmer)\n",
    "    summary = tensor_to_string(x[key2])\n",
    "    summary = get_sent_list(summary,stemmer)\n",
    "    \n",
    "    filename = str(index) + \"_\" + model + \"_\" + stemmer + \".pickle\"\n",
    "    folderpath = os.path.join(\"/mnt/disks/disk-1/data/pickle\",kind)\n",
    "    filepath = os.path.join(\"/mnt/disks/disk-1/data/pickle\",kind,filename)\n",
    "    \n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, 'rb') as handle:\n",
    "            text_emb = pickle.load(handle)\n",
    "    else:\n",
    "        text_emb = models[model].encode(text)\n",
    "        with open(filepath, 'wb') as handle:\n",
    "            pickle.dump(text_emb, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    gen_sum = [text[x] for x in generate_summary(text_emb,len(summary))]\n",
    "    scores = scorer.score(\" \".join(summary),\" \".join(gen_sum))\n",
    "    return scores[\"rouge1\"].fmeasure, scores[\"rouge2\"].fmeasure, scores[\"rougeL\"].fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5740baad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /mnt/disks/disk-1/data/cnn_dailymail/3.1.0\n",
      "INFO:absl:Reusing dataset cnn_dailymail (/mnt/disks/disk-1/data/cnn_dailymail/3.1.0)\n",
      "INFO:absl:Constructing tf.data.Dataset cnn_dailymail for split ['train', 'validation', 'test'], from /mnt/disks/disk-1/data/cnn_dailymail/3.1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "cnn_dailymail multi-qa-mpnet-base-dot-v1\n",
      "11490\n",
      "Rouge 1 :  33.01\n",
      "Rouge 2 :  12.01\n",
      "Rouge L :  20.87\n",
      "___\n",
      "CPU times: user 4min 52s, sys: 1.11 s, total: 4min 53s\n",
      "Wall time: 4min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# datasets = [\"cnn_dailymail\",\"scientific_papers/arxiv\",\"scientific_papers/pubmed\"]\n",
    "datasets = [\"cnn_dailymail\"]\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2','rougeL'], use_stemmer=True)\n",
    "for ds in datasets:\n",
    "    for trans in transformers:\n",
    "        train, val, test = tfds.load(name=ds, \n",
    "                              split=[\"train\", \"validation\", \"test\"], \n",
    "                              data_dir=\"/mnt/disks/disk-1/data\")\n",
    "        \n",
    "#         model = SentenceTransformer(trans,cache_folder='/mnt/disks/disk-1/data/models')\n",
    "#         model._target_device = device\n",
    "        r1 = []\n",
    "        r2 = []\n",
    "        rl = []\n",
    "        index = 0\n",
    "        for x in list(test):\n",
    "            r1_val,r2_val,rl_val = uml_summary(x,index,kind=ds,model=trans)\n",
    "            index += 1\n",
    "            r1.append(r1_val)\n",
    "            r2.append(r2_val)\n",
    "            rl.append(rl_val)\n",
    "            if index % 1000 == 0 and index > 0:\n",
    "                print(index)\n",
    "        print(ds,trans)\n",
    "        print(index)\n",
    "        print(\"Rouge 1 : \",np.round(np.mean(np.asarray(r1))*100,2))\n",
    "        print(\"Rouge 2 : \",np.round(np.mean(np.asarray(r2))*100,2))\n",
    "        print(\"Rouge L : \",np.round(np.mean(np.asarray(rl))*100,2))\n",
    "        print(\"___\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b60eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1129fe45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cb64c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabb490a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17674a02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3c9094",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
