{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d531a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import struct\n",
    "import hashlib\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "import tensorflow_hub as hub\n",
    "import tokenization\n",
    "from sklearn.manifold import TSNE\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.cluster import KMeans\n",
    "import pickle\n",
    "from sknetwork.ranking import PageRank\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import torch\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2141e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_string(x):\n",
    "    return x.numpy().decode('UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "319e7aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_list(text,stem=None):\n",
    "    sents = sent_tokenize(text)\n",
    "    if stem == \"None\":\n",
    "        return sents\n",
    "    if stem == \"EnglishStemmer\":\n",
    "        stemmer = EnglishStemmer()\n",
    "    ans = []\n",
    "    for sent in sents:\n",
    "        words = word_tokenize(sent)\n",
    "        word_stem = [stemmer.stem(w) for w in words]\n",
    "        ans.append(str(\" \".join(word_stem)))\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbdb3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e86d4702",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Model features include an encode function -> takes a list of sentences. Returns a list of embeddings (all same dim)\n",
    "transformers = [\"paraphrase-albert-small-v2\"]\n",
    "\n",
    "models = dict()\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for trans in transformers:\n",
    "    models[trans] = SentenceTransformer(trans,cache_folder='/mnt/disks/disk-1/data/models')\n",
    "    models[trans]._target_device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d7b4dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## V = list of embeddings. k = target size of summary\n",
    "## Returns a list of sentence indices\n",
    "\n",
    "def generate_summary(V, k):\n",
    "    if k >= len(V):\n",
    "        return list(range(len(V)))\n",
    "    k -= 1\n",
    "    centers = []\n",
    "    cities = list(range(len(V)))\n",
    "    centers.append(0)\n",
    "    cities.remove(0)\n",
    "    while k!= 0:\n",
    "        city_dict = {}\n",
    "        for cty in cities:\n",
    "            min_dist = float(\"inf\")\n",
    "            for c in centers:\n",
    "                min_dist = min(min_dist,np.linalg.norm(V[cty] - V[c]))\n",
    "            city_dict[cty] = min_dist\n",
    "        new_center = max(city_dict, key = lambda i: city_dict[i])\n",
    "        centers.append(new_center)\n",
    "        cities.remove(new_center)\n",
    "        k -= 1\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fc014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pagerank version\n",
    "\n",
    "def sim(a, b):\n",
    "    return np.dot(a, b) / np.sqrt(np.dot(a, a) * np.dot(b, b))\n",
    "\n",
    "def generate_summary(V,k):\n",
    "    if k >= len(V):\n",
    "        return list(range(len(V)))\n",
    "    n = V.shape[0]\n",
    "    adj = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        adj[i][i] = sim(V[i],V[i])\n",
    "        for j in range(i+1,n):\n",
    "            s = sim(V[i], V[j])\n",
    "            adj[i][j] = s\n",
    "            adj[j][i] = s\n",
    "\n",
    "    pr = PageRank()\n",
    "    scores = pr.fit_transform(adj)\n",
    "    ind = np.argpartition(scores, -k)[-k:]\n",
    "    return np.sort(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b415b8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uml_summary(x,index,kind=\"cnn_dailymail\",model=\"all-MiniLM-L6-v2\"):\n",
    "    if kind == \"cnn_dailymail\":\n",
    "        key1 = 'article'\n",
    "        key2 = 'highlights'\n",
    "    elif kind == \"scientific_papers/arxiv\" or kind == \"scientific_papers/pubmed\":\n",
    "        key1 = 'article'\n",
    "        key2 = 'abstract'\n",
    "        \n",
    "    stemmer = \"EnglishStemmer\"\n",
    "    text = tensor_to_string(x[key1])\n",
    "    text = get_sent_list(text,stemmer)\n",
    "    summary = tensor_to_string(x[key2])\n",
    "    summary = get_sent_list(summary,stemmer)\n",
    "    \n",
    "    filename = str(index) + \"_\" + model + \".pickle\"\n",
    "    folderpath = os.path.join(\"/mnt/disks/disk-1/data/pickle\",kind)\n",
    "    filepath = os.path.join(\"/mnt/disks/disk-1/data/pickle\",kind,filename)\n",
    "    \n",
    "    text_emb = models[model].encode(text)\n",
    "#     if os.path.exists(filepath):\n",
    "#         with open(filepath, 'rb') as handle:\n",
    "#             text_emb = pickle.load(handle)\n",
    "#     else:\n",
    "#         text_emb = models[model].encode(text)\n",
    "#         with open(filepath, 'wb') as handle:\n",
    "#             pickle.dump(text_emb, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    gen_sum = [text[x] for x in generate_summary(text_emb,len(summary))]\n",
    "    scores = scorer.score(\" \".join(summary),\" \".join(gen_sum))\n",
    "    return scores[\"rouge1\"].fmeasure, scores[\"rouge2\"].fmeasure, scores[\"rougeL\"].fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5740baad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /mnt/disks/disk-1/data/scientific_papers/arxiv/1.1.1\n",
      "INFO:absl:Reusing dataset scientific_papers (/mnt/disks/disk-1/data/scientific_papers/arxiv/1.1.1)\n",
      "INFO:absl:Constructing tf.data.Dataset scientific_papers for split ['train', 'validation', 'test'], from /mnt/disks/disk-1/data/scientific_papers/arxiv/1.1.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "101\n",
      "scientific_papers/arxiv paraphrase-albert-small-v2\n",
      "101\n",
      "Rouge 1 :  19.56\n",
      "Rouge 2 :  4.37\n",
      "Rouge L :  12.77\n",
      "___\n",
      "CPU times: user 27min 9s, sys: 1min 44s, total: 28min 54s\n",
      "Wall time: 5min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# datasets = [\"cnn_dailymail\",\"scientific_papers/arxiv\",\"scientific_papers/pubmed\"]\n",
    "datasets = [\"cnn_dailymail\"]\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2','rougeL'], use_stemmer=True)\n",
    "for ds in datasets:\n",
    "    for trans in transformers:\n",
    "        train, val, test = tfds.load(name=ds, \n",
    "                              split=[\"train\", \"validation\", \"test\"], \n",
    "                              data_dir=\"/mnt/disks/disk-1/data\")\n",
    "        \n",
    "#         model = SentenceTransformer(trans,cache_folder='/mnt/disks/disk-1/data/models')\n",
    "#         model._target_device = device\n",
    "        r1 = []\n",
    "        r2 = []\n",
    "        rl = []\n",
    "        index = 0\n",
    "        for x in list(test):\n",
    "            r1_val,r2_val,rl_val = uml_summary(x,index,kind=ds,model=trans)\n",
    "            index += 1\n",
    "            r1.append(r1_val)\n",
    "            r2.append(r2_val)\n",
    "            rl.append(rl_val)\n",
    "            print(index)\n",
    "            if index > 100:\n",
    "                print(index)\n",
    "                break\n",
    "        print(ds,trans)\n",
    "        print(index)\n",
    "        print(\"Rouge 1 : \",np.round(np.mean(np.asarray(r1))*100,2))\n",
    "        print(\"Rouge 2 : \",np.round(np.mean(np.asarray(r2))*100,2))\n",
    "        print(\"Rouge L : \",np.round(np.mean(np.asarray(rl))*100,2))\n",
    "        print(\"___\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18b60eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3402489626556016 0.058577405857740586 0.1908713692946058\n"
     ]
    }
   ],
   "source": [
    "kind=\"scientific_papers/arxiv\"\n",
    "model=\"paraphrase-albert-small-v2\"\n",
    "if kind == \"cnn_dailymail\":\n",
    "    key1 = 'article'\n",
    "    key2 = 'highlights'\n",
    "elif kind == \"scientific_papers/arxiv\" or kind == \"scientific_papers/pubmed\":\n",
    "    key1 = 'article'\n",
    "    key2 = 'abstract'\n",
    "\n",
    "stemmer = \"EnglishStemmer\"\n",
    "text = tensor_to_string(x[key1])\n",
    "text = get_sent_list(text,stemmer)\n",
    "summary = tensor_to_string(x[key2])\n",
    "summary = get_sent_list(summary,stemmer)\n",
    "\n",
    "text_emb = models[model].encode(text)\n",
    "\n",
    "gen_sum = [text[x] for x in generate_summary(text_emb,len(summary))]\n",
    "scores = scorer.score(\" \".join(summary),\" \".join(gen_sum))\n",
    "print(scores[\"rouge1\"].fmeasure, scores[\"rouge2\"].fmeasure, scores[\"rougeL\"].fmeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1129fe45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['linear wave function in quantum chemistri are fundament limit by their inabl to compact express wave function in strong correl regim , a difficulti that aris direct from the factori growth of hilbert space in the quantum mani - bodi problem .',\n",
       " 'what if the plane from one bond intersect a far - away atom ?',\n",
       " 'the nodal surfac of @ xmath38 is a plane center at @ xmath39 and normal to the unit vector @ xmath40 , while the nodal surfac of @ xmath41 is an ellipsoid with center @ xmath39 and axe defin by the eigenvector and eigenvalu of @ xmath42 .']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3cb64c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we demonstr that 4-bodi real space jastrow factor are , with the right type of jastrow basi function , capabl of perform success wave function stencil to remov unwant ionic term from an overabund fermion refer without unduli modifi the remain compon .',\n",
       " 'in addit to great improv size consist ( restor it exact in the case of a gemin power ) , real - space wave function stencil is , unlik it hilbert space predecessor , immedi compat with diffus mont carlo , allow it to be use in the pursuit of compact , strong correl trial function with reliabl nodal surfac .',\n",
       " 'we demonstr the efficaci of this approach in the context of a doubl bond dissoci by use it to extract a qualit correct nodal surfac despit be pair with a restrict slater determin , that , due to ionic term error , produc a ground state with a qualit incorrect nodal surfac when use in the absenc of the jastrow .']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabb490a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17674a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21212121212121213 0.03125 0.1212121212121212\n"
     ]
    }
   ],
   "source": [
    "kind=\"scientific_papers/arxiv\"\n",
    "model=\"paraphrase-albert-small-v2\"\n",
    "if kind == \"cnn_dailymail\":\n",
    "    key1 = 'article'\n",
    "    key2 = 'highlights'\n",
    "elif kind == \"scientific_papers/arxiv\" or kind == \"scientific_papers/pubmed\":\n",
    "    key1 = 'article'\n",
    "    key2 = 'abstract'\n",
    "\n",
    "stemmer = \"EnglishStemmer\"\n",
    "text = tensor_to_string(x[key1])\n",
    "text = get_sent_list(text,stemmer)\n",
    "summary = tensor_to_string(x[key2])\n",
    "summary = get_sent_list(summary,stemmer)\n",
    "\n",
    "text_emb = models[model].encode(text)\n",
    "\n",
    "gen_sum = [text[x] for x in generate_summary(text_emb,len(summary))]\n",
    "scores = scorer.score(\" \".join(summary),\" \".join(gen_sum))\n",
    "print(scores[\"rouge1\"].fmeasure, scores[\"rouge2\"].fmeasure, scores[\"rougeL\"].fmeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6e28a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a mother-of-two lost more than ten stone after she becam concern that her size was caus her to look like a man .',\n",
       " 'snack : crisp .',\n",
       " 'dinner : chicken stir-fri .']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e4dbe2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kim callaghan , from ireland , pile on the pound after have children .',\n",
       " 'limit to size 28 cloth kim , 39 , worri she resembl a man .',\n",
       " 'she join slim world and drop ten dress size as well as 10st .']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9966f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "800d44cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/mnt/disks/disk-1/data/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc1497f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn',cache_dir='/mnt/disks/disk-1/data/models')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn',cache_dir='/mnt/disks/disk-1/data/models')\n",
    "\n",
    "ARTICLE_TO_SUMMARIZE = \"My friends are cool but they eat too many carbs.\"\n",
    "inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, truncation=True,return_tensors='tf')\n",
    "\n",
    "# Generate Summary\n",
    "# summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=5, early_stopping=True)\n",
    "# print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9fb27b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b37827cf8734ae2bba2bdd6f0a908a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/775M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: \n",
      "INFO:absl:Unable to initialize backend 'gpu': NOT_FOUND: Could not find registered platform with name: \"cuda\". Available platform names are: Interpreter Host\n",
      "INFO:absl:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "Some weights of FlaxBartForConditionalGeneration were not initialized from the model checkpoint at facebook/bart-large and are newly initialized: {('final_logits_bias',)}\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBartModel,FlaxBartForConditionalGeneration,BartTokenizer\n",
    "model = FlaxBartForConditionalGeneration.from_pretrained('facebook/bart-large',cache_dir='/mnt/disks/disk-1/data/models')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large',cache_dir='/mnt/disks/disk-1/data/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "205e952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"My friends are cool but they eat too many carbs.\",\"My friends are cool but they eat too many carbs.\"]\n",
    "inputs = tokenizer(text, max_length=1024, return_tensors='jax')\n",
    "encoder_outputs = model.encode(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c03b7a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 1024)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = list(encoder_outputs.last_hidden_state)\n",
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "edfd165e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of FlaxBartForConditionalGeneration were not initialized from the model checkpoint at facebook/bart-large and are newly initialized: {('final_logits_bias',)}\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = FlaxBartForConditionalGeneration.from_pretrained('facebook/bart-large',cache_dir='/mnt/disks/disk-1/data/models')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large',cache_dir='/mnt/disks/disk-1/data/models')\n",
    "\n",
    "def uml_summary2(x,index,kind=\"cnn_dailymail\"):\n",
    "    if kind == \"cnn_dailymail\":\n",
    "        key1 = 'article'\n",
    "        key2 = 'highlights'\n",
    "    elif kind == \"scientific_papers/arxiv\" or kind == \"scientific_papers/pubmed\":\n",
    "        key1 = 'article'\n",
    "        key2 = 'abstract'\n",
    "        \n",
    "    stemmer = \"EnglishStemmer\"\n",
    "    text = tensor_to_string(x[key1])\n",
    "    text = get_sent_list(text,stemmer)\n",
    "    summary = tensor_to_string(x[key2])\n",
    "    summary = get_sent_list(summary,stemmer)\n",
    "    \n",
    "    inputs = tokenizer(text, max_length=1024, return_tensors='jax',padding=True)\n",
    "    text_emb = model.encode(**inputs)\n",
    "    text_emb = list(model.encode(**inputs).last_hidden_state)\n",
    "#     if os.path.exists(filepath):\n",
    "#         with open(filepath, 'rb') as handle:\n",
    "#             text_emb = pickle.load(handle)\n",
    "#     else:\n",
    "#         text_emb = models[model].encode(text)\n",
    "#         with open(filepath, 'wb') as handle:\n",
    "#             pickle.dump(text_emb, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    gen_sum = [text[x] for x in generate_summary(text_emb,len(summary))]\n",
    "    scores = scorer.score(\" \".join(summary),\" \".join(gen_sum))\n",
    "    return scores[\"rouge1\"].fmeasure, scores[\"rouge2\"].fmeasure, scores[\"rougeL\"].fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "17d2f6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /mnt/disks/disk-1/data/cnn_dailymail/3.1.0\n",
      "INFO:absl:Reusing dataset cnn_dailymail (/mnt/disks/disk-1/data/cnn_dailymail/3.1.0)\n",
      "INFO:absl:Constructing tf.data.Dataset cnn_dailymail for split ['train', 'validation', 'test'], from /mnt/disks/disk-1/data/cnn_dailymail/3.1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "101\n",
      "cnn_dailymail paraphrase-albert-small-v2\n",
      "101\n",
      "Rouge 1 :  30.06\n",
      "Rouge 2 :  10.52\n",
      "Rouge L :  17.89\n",
      "___\n",
      "CPU times: user 49min 16s, sys: 16.3 s, total: 49min 33s\n",
      "Wall time: 15min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# datasets = [\"cnn_dailymail\",\"scientific_papers/arxiv\",\"scientific_papers/pubmed\"]\n",
    "datasets = [\"cnn_dailymail\"]\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2','rougeL'], use_stemmer=True)\n",
    "for ds in datasets:\n",
    "    for trans in transformers:\n",
    "        train, val, test = tfds.load(name=ds, \n",
    "                              split=[\"train\", \"validation\", \"test\"], \n",
    "                              data_dir=\"/mnt/disks/disk-1/data\")\n",
    "        \n",
    "#         model = SentenceTransformer(trans,cache_folder='/mnt/disks/disk-1/data/models')\n",
    "#         model._target_device = device\n",
    "        r1 = []\n",
    "        r2 = []\n",
    "        rl = []\n",
    "        index = 0\n",
    "        for x in list(test):\n",
    "            r1_val,r2_val,rl_val = uml_summary2(x,index,kind=ds)\n",
    "            index += 1\n",
    "            r1.append(r1_val)\n",
    "            r2.append(r2_val)\n",
    "            rl.append(rl_val)\n",
    "            print(index)\n",
    "            if index > 100:\n",
    "                print(index)\n",
    "                break\n",
    "        print(ds,trans)\n",
    "        print(index)\n",
    "        print(\"Rouge 1 : \",np.round(np.mean(np.asarray(r1))*100,2))\n",
    "        print(\"Rouge 2 : \",np.round(np.mean(np.asarray(r2))*100,2))\n",
    "        print(\"Rouge L : \",np.round(np.mean(np.asarray(rl))*100,2))\n",
    "        print(\"___\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c9c2a929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /mnt/disks/disk-1/data/cnn_dailymail/3.1.0\n",
      "INFO:absl:Reusing dataset cnn_dailymail (/mnt/disks/disk-1/data/cnn_dailymail/3.1.0)\n",
      "INFO:absl:Constructing tf.data.Dataset cnn_dailymail for split ['train', 'validation', 'test'], from /mnt/disks/disk-1/data/cnn_dailymail/3.1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "101\n",
      "cnn_dailymail paraphrase-albert-small-v2\n",
      "101\n",
      "Rouge 1 :  32.21\n",
      "Rouge 2 :  10.72\n",
      "Rouge L :  20.08\n",
      "___\n",
      "CPU times: user 3min 27s, sys: 1.31 s, total: 3min 28s\n",
      "Wall time: 36.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# datasets = [\"cnn_dailymail\",\"scientific_papers/arxiv\",\"scientific_papers/pubmed\"]\n",
    "datasets = [\"cnn_dailymail\"]\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2','rougeL'], use_stemmer=True)\n",
    "for ds in datasets:\n",
    "    for trans in transformers:\n",
    "        train, val, test = tfds.load(name=ds, \n",
    "                              split=[\"train\", \"validation\", \"test\"], \n",
    "                              data_dir=\"/mnt/disks/disk-1/data\")\n",
    "        \n",
    "#         model = SentenceTransformer(trans,cache_folder='/mnt/disks/disk-1/data/models')\n",
    "#         model._target_device = device\n",
    "        r1 = []\n",
    "        r2 = []\n",
    "        rl = []\n",
    "        index = 0\n",
    "        for x in list(test):\n",
    "            r1_val,r2_val,rl_val = uml_summary(x,index,kind=ds,model=trans)\n",
    "            index += 1\n",
    "            r1.append(r1_val)\n",
    "            r2.append(r2_val)\n",
    "            rl.append(rl_val)\n",
    "            print(index)\n",
    "            if index > 100:\n",
    "                print(index)\n",
    "                break\n",
    "        print(ds,trans)\n",
    "        print(index)\n",
    "        print(\"Rouge 1 : \",np.round(np.mean(np.asarray(r1))*100,2))\n",
    "        print(\"Rouge 2 : \",np.round(np.mean(np.asarray(r2))*100,2))\n",
    "        print(\"Rouge L : \",np.round(np.mean(np.asarray(rl))*100,2))\n",
    "        print(\"___\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa7780af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11490"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0959048b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() missing 1 required positional argument: 'train_objectives'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31722/4284004348.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: fit() missing 1 required positional argument: 'train_objectives'"
     ]
    }
   ],
   "source": [
    "models[trans].fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3c9094",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
